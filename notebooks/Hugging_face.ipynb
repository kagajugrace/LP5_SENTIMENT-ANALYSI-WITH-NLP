{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richard-Mensah/SENTIMENT-ANALYSI-WITH-NLP/blob/main/Hugging_face_richardmensah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2exvBPRM5Fr"
      },
      "source": [
        "# Sentiment Analysis with Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyqMa46fM5GR"
      },
      "source": [
        "Hugging Face is an open-source and platform provider of machine learning technologies. You can use install their package to access some interesting pre-built models to use them directly or to fine-tune (retrain it on your dataset leveraging the prior knowledge coming with the first training), then host your trained models on the platform, so that you may use them later on other devices and apps.\n",
        "\n",
        "Please, [go to the website and sign-in](https://huggingface.co/) to access all the features of the platform.\n",
        "\n",
        "[Read more about Text classification with Hugging Face](https://huggingface.co/tasks/text-classification)\n",
        "\n",
        "The Hugging face models are Deep Learning based, so will need a lot of computational GPU power to train them. Please use [Colab](https://colab.research.google.com/) to do it, or your other GPU cloud provider, or a local machine having NVIDIA GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnAVmO_eM5GU"
      },
      "source": [
        "## Application of Hugging Face Text classification model Fune-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOmGEPGfM5GV"
      },
      "source": [
        "Find below a simple example, with just `3 epochs of fine-tuning`. \n",
        "\n",
        "Read more about the fine-tuning concept : [here](https://deeplizard.com/learn/video/5T-iXNNiwIs#:~:text=Fine%2Dtuning%20is%20a%20way,perform%20a%20second%20similar%20task.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFwEZUDEq0cE"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxPcvIFNqx_K"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrxIHtodqxAQ"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hkg-TfCM5GX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "# from scipy.special import softmax\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "# from transformers import HubManager\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj0eV5YUM5Ga"
      },
      "outputs": [],
      "source": [
        "# Disabe W&B\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ6MmoQzM5Gb"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from a GitHub link\n",
        "url = \"https://raw.githubusercontent.com/Azubi-Africa/Career_Accelerator_P5-NLP/master/zindi_challenge/data/Train.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# A way to eliminate rows containing NaN values\n",
        "df = df[~df.isna().any(axis=1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebVVDR2pM5Gc"
      },
      "source": [
        "I manually split the training set to have a training subset ( a dataset the model will learn on), and an evaluation subset ( a dataset the model with use to compute metric scores to help use to avoid some training problems like [the overfitting](https://www.ibm.com/cloud/learn/overfitting) one ). \n",
        "\n",
        "There are multiple ways to do split the dataset. You'll see two commented line showing you another one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F8L25--M5Ge"
      },
      "outputs": [],
      "source": [
        "# Split the train data => {train, eval}\n",
        "train, eval = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m6lssvoM5Gh"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en8ZlNb_M5Gl"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSSOm3-2M5Gn"
      },
      "outputs": [],
      "source": [
        "eval.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USbmZj8rM5Go"
      },
      "outputs": [],
      "source": [
        "eval.label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFuOwIrCM5Gp"
      },
      "outputs": [],
      "source": [
        "print(f\"new dataframe shapes: train is {train.shape}, eval is {eval.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmQq8A4SM5Gq"
      },
      "outputs": [],
      "source": [
        "# # Save splitted subsets\n",
        "# train.to_csv(\"../data/train_subset.csv\", index=False)\n",
        "# eval.to_csv(\"../data/eval_subset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_FpiKwMM5Gr"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict, Dataset\n",
        "train_dataset = Dataset.from_pandas(train[['tweet_id', 'safe_text', 'label', 'agreement']])\n",
        "eval_dataset = Dataset.from_pandas(eval[['tweet_id', 'safe_text', 'label', 'agreement']])\n",
        "\n",
        "dataset = DatasetDict({'train': train_dataset, 'eval': eval_dataset})\n",
        "dataset = dataset.remove_columns('__index_level_0__')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GQPM1DAgOYo"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6vIJTopM5Gs"
      },
      "outputs": [],
      "source": [
        "# Preprocess text (username and link placeholders)\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "checkpoint = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# config = AutoConfig.from_pretrained(MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2kaE8v_M5Gt"
      },
      "outputs": [],
      "source": [
        "def transform_labels(label):\n",
        "\n",
        "    label = label['label']\n",
        "    num = 0\n",
        "    if label == -1: #'Negative'\n",
        "        num = 0\n",
        "    elif label == 0: #'Neutral'\n",
        "        num = 1\n",
        "    elif label == 1: #'Positive'\n",
        "        num = 2\n",
        "\n",
        "    return {'labels': num}\n",
        "\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['safe_text'], padding='max_length')\n",
        "\n",
        "# Change the tweets to tokens that the models can exploit\n",
        "dataset = dataset.map(tokenize_data, batched=True)\n",
        "\n",
        "# Transform\tlabels and remove the useless columns\n",
        "remove_columns = ['tweet_id', 'label', 'safe_text', 'agreement']\n",
        "dataset = dataset.map(transform_labels, remove_columns=remove_columns)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6qG7GhVM5Gy"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl9UEr1qM5G0"
      },
      "outputs": [],
      "source": [
        "# dataset['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYXA4zDKM5G1"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckHDKxDPnyWl"
      },
      "outputs": [],
      "source": [
        "pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV_vmBAGM5G1"
      },
      "outputs": [],
      "source": [
        "# Configure the trianing parameters like `num_train_epochs`: \n",
        "# the number of time the model will repeat the training loop over the dataset\n",
        "training_args = TrainingArguments(\"test_trainer\", \n",
        "                                  num_train_epochs=10, \n",
        "                                  load_best_model_at_end=True, \n",
        "                                  save_strategy='epoch',\n",
        "                                  evaluation_strategy='epoch',\n",
        "                                  logging_strategy='epoch',\n",
        "                                  logging_steps=100,\n",
        "                                  per_device_train_batch_size=8,\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a643fStBM5G3"
      },
      "outputs": [],
      "source": [
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cUkWOFpM5G4"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset['train'].shuffle(seed=10) \n",
        "eval_dataset = dataset['eval'].shuffle(seed=10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cixKFZeOM5G5"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"rmse\": mean_squared_error(labels, predictions, squared=False)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoNNWPz_M5G6"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args, \n",
        "    train_dataset=train_dataset, \n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XEh48vVzYdP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtsQqZJGM5G7"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjVR-ilCM5G8"
      },
      "source": [
        "Don't worry the above issue, it is a `KeyboardInterrupt` that means I stopped the training to avoid taking a long time to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ8w2aNBM5G9"
      },
      "outputs": [],
      "source": [
        "# Launch the final evaluation \n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHY9rVqfM5G-"
      },
      "source": [
        "Some checkpoints of the model are automatically saved locally in `test_trainer/` during the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnGHAwdLM5HA"
      },
      "source": [
        "You may also upload the model on the Hugging Face Platform... [Read more](https://huggingface.co/docs/hub/models-uploading)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcdYCVYCM5HA"
      },
      "outputs": [],
      "source": [
        "# Push the model and tokenizer to Hugging Face\n",
        "token = \"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\"\n",
        "model.push_to_hub(\"GraceKagaju/twitter_xlm_roberta_base\", use_auth_token=token, commit_message=\"Pushed model\")\n",
        "tokenizer.push_to_hub(\"GraceKagaju/twitter_xlm_roberta_base\", use_auth_token=token, commit_message=\"pushed tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iys7gWEm0HK4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1ab24538aa0da4b2d8c48eaca591ff7ffc54671225fb0511b432fd9e26a098ba"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
